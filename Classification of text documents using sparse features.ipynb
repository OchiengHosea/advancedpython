{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying documents using a bag-of-words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import NearestCentroid, KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Usage: ipykernel_launcher.py [options]\n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --report              Print a detailed classification report.\n",
      "  --chi2_select=SELECT_CHI2\n",
      "                        Select some number of features using chi-squared test\n",
      "  --confusion matrix    Print the confusion matrix\n",
      "  --top10               Print ten most discriminative terms per class for\n",
      "                        every classifier\n",
      "  --all_categories      Whether to use all categories or not\n",
      "  --use_hashing         Use hashing vectorizer\n",
      "  --n_features=N_FEATURES\n",
      "                        n_featurs when using hashing vectorizer.\n",
      "  --filtered            Remove newsgroup information that is easily overfit:\n",
      "                        headers, signatures, and quoting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime) %(levelname) %(message)s')\n",
    "\n",
    "op = OptionParser()\n",
    "op.add_option('--report', action=\"store_true\", dest=\"print_report\", help=\"Print a detailed classification report.\")\n",
    "op.add_option('--chi2_select', action=\"store\", type=\"int\", dest=\"select_chi2\", help=\"Select some number of features using chi-squared test\")\n",
    "op.add_option('--confusion matrix', action=\"store_true\", dest=\"print_cm\", help=\"Print the confusion matrix\")\n",
    "op.add_option(\"--top10\", action=\"store_true\", dest=\"print_top10\", help=\"Print ten most discriminative terms per class for every classifier\")\n",
    "op.add_option(\"--all_categories\", action=\"store_true\", dest=\"all_categories\", help=\"Whether to use all categories or not\")\n",
    "op.add_option(\"--use_hashing\", action=\"store_true\", help=\"Use hashing vectorizer\")\n",
    "op.add_option(\"--n_features\", action=\"store\", type=\"int\", help=\"n_featurs when using hashing vectorizer.\")\n",
    "op.add_option(\"--filtered\", action=\"store_true\", help=\"Remove newsgroup information that is easily overfit: headers, signatures, and quoting\")\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)\n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from the training set\n",
    "Load data from the newsgroup dataset which comprises around 18000 newsgroups posts on 20 topics split in two subsets, one for training and the other for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "if opts.all_categories:\n",
    "    categories = None\n",
    "else:\n",
    "    categories = [\n",
    "        'alt.atheism',\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "        'sci.space'\n",
    "    ]\n",
    "    \n",
    "if opts.filtered:\n",
    "    remove = ('headers', 'footers', 'quotes')\n",
    "else:\n",
    "    remove = ()\n",
    "    \n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories if categories else \"all\")\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "\n",
    "print('data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 documents - 3.980MB (training set)\n",
      "1353 documents - 2.867MB (testing set)\n",
      "4 categories \n",
      "\n",
      "Extracting features from the training data using a sparse vectorizer\n",
      "done in 0.396706s at 10.031MB/s\n",
      "n_samples: 2034, n_features: 33809\n",
      "\n",
      "Extracting features from the test data using the same vectorizer\n",
      "done in 0.236674s at 12.116MB/s\n",
      "n_samples: 1353, n_features: 33809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = data_train.target_names\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "data_train_size_mb = size_mb(data_train.data)\n",
    "data_test_size_mb = size_mb(data_test.data)\n",
    "\n",
    "print('%d documents - %0.3fMB (training set)' % (len(data_train.data), data_train_size_mb))\n",
    "print('%d documents - %0.3fMB (testing set)' % (len(data_test.data), data_test_size_mb))\n",
    "print(\"%d categories \" % len(target_names))\n",
    "print()\n",
    "\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    vectorizer = HashingVectorizer(stop_words='english', alternate_sign=False, n_features=opts.n_featurs)\n",
    "    X_train = vectorizer.transform(data_train.data)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "    X_train = vectorizer.fit_transform(data_train.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from integer feature name to original token string\n",
    "if opts.use_hashing:\n",
    "    feature_names = None\n",
    "else:\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "if opts.select_chi2:\n",
    "    print(\"Extracting %d best features by a chi-squared test \" %opt.select_chi2)\n",
    "    t0 = time()\n",
    "    ch2 = SelectGest(chi2, k=opts.select_chi2)\n",
    "    X_train = ch2.fir_transform(X_train, y_train)\n",
    "    X_test = ch2.transform(X_test)\n",
    "    if feature_names:\n",
    "        # kee selected feature names\n",
    "        feature_names = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "    print(\"Done in %fs\" % time() - t0)\n",
    "    print()\n",
    "    \n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "    \n",
    "def trim(s):\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Classifiers\n",
    "We train classifiers with 15 different classification models and get performance results for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "RidgeClassifier(solver='sag', tol=0.01)\n",
      "Train time: 0.175s\n",
      "test time: 0.002s\n",
      "accuracy: 0.897\n",
      "Dimensionality: 33809\n",
      "Density: 1.000000\n",
      "\n",
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "Perceptron(max_iter=50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duke/duke/programming/advancedpython/venv/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:555: UserWarning: \"sag\" solver requires many iterations to fit an intercept with sparse inputs. Either set the solver to \"auto\" or \"sparse_cg\", or set a low \"tol\" and a high \"max_iter\" (especially if inputs are not standardized).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 0.037s\n",
      "test time: 0.004s\n",
      "accuracy: 0.888\n",
      "Dimensionality: 33809\n",
      "Density: 0.255302\n",
      "\n",
      "================================================================================\n",
      "Passive-Aggressive\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "PassiveAggressiveClassifier(max_iter=50)\n",
      "Train time: 0.069s\n",
      "test time: 0.004s\n",
      "accuracy: 0.902\n",
      "Dimensionality: 33809\n",
      "Density: 0.701611\n",
      "\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "KNeighborsClassifier(n_neighbors=10)\n",
      "Train time: 0.002s\n",
      "test time: 0.234s\n",
      "accuracy: 0.858\n",
      "================================================================================\n",
      "Random Forest\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "RandomForestClassifier()\n",
      "Train time: 1.347s\n",
      "test time: 0.076s\n",
      "accuracy: 0.838\n",
      "================================================================================\n",
      "L1 penalty\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "LinearSVC(dual=False, penalty='l1', tol=0.001)\n",
      "Train time: 0.194s\n",
      "test time: 0.001s\n",
      "accuracy: 0.873\n",
      "Dimensionality: 33809\n",
      "Density: 0.005553\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "SGDClassifier(max_iter=50, penalty='l1')\n",
      "Train time: 0.226s\n",
      "test time: 0.004s\n",
      "accuracy: 0.883\n",
      "Dimensionality: 33809\n",
      "Density: 0.022768\n",
      "\n",
      "================================================================================\n",
      "L2 penalty\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "LinearSVC(dual=False, tol=0.001)\n",
      "Train time: 0.428s\n",
      "test time: 0.003s\n",
      "accuracy: 0.900\n",
      "Dimensionality: 33809\n",
      "Density: 1.000000\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "SGDClassifier(max_iter=50)\n",
      "Train time: 0.068s\n",
      "test time: 0.004s\n",
      "accuracy: 0.902\n",
      "Dimensionality: 33809\n",
      "Density: 0.568221\n",
      "\n",
      "================================================================================\n",
      "Nearestcentroid (aka Rocchio classifier)\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "NearestCentroid()\n",
      "Train time: 0.008s\n",
      "test time: 0.005s\n",
      "accuracy: 0.855\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "MultinomialNB(alpha=0.01)\n",
      "Train time: 0.010s\n",
      "test time: 0.004s\n",
      "accuracy: 0.899\n",
      "Dimensionality: 33809\n",
      "Density: 1.000000\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "BernoulliNB(alpha=0.1)\n",
      "Train time: 0.012s\n",
      "test time: 0.012s\n",
      "accuracy: 0.877\n",
      "Dimensionality: 33809\n",
      "Density: 1.000000\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "ComplementNB(alpha=0.1)\n",
      "Train time: 0.010s\n",
      "test time: 0.004s\n",
      "accuracy: 0.911\n",
      "Dimensionality: 33809\n",
      "Density: 1.000000\n",
      "\n",
      "================================================================================\n",
      "LinearSVC with L1-based feature selection\n",
      "________________________________________________________________________________\n",
      "Training\n",
      "Pipeline(steps=[('feature_selection',\n",
      "                 SelectFromModel(estimator=LinearSVC(dual=False, penalty='l1',\n",
      "                                                     tol=0.001))),\n",
      "                ('classification', LinearSVC())])\n",
      "Train time: 0.225s\n",
      "test time: 0.002s\n",
      "accuracy: 0.879\n"
     ]
    }
   ],
   "source": [
    "def benchmark(clf):\n",
    "    print(\"_\"*80)\n",
    "    print(\"Training\")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"Train time: %0.3fs\" % train_time)\n",
    "    \n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time: %0.3fs\" %test_time)\n",
    "    \n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy: %0.3f\" % score)\n",
    "    \n",
    "    if hasattr(clf, \"coef_\"):\n",
    "        print(\"Dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"Density: %f\" %density(clf.coef_))\n",
    "        \n",
    "        if opts.print_top10 and feature_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate(target_names):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "            print()\n",
    "        if opts.print_report:\n",
    "            print(\"classification report:\")\n",
    "            print(metrics.classification_report(y_test, pred, target_names=target_names))\n",
    "        \n",
    "        if opts.print_cm:\n",
    "            print(\"confusion matrix\")\n",
    "            print(metric.confusion_matrix(y_test, pred))\n",
    "        \n",
    "        print()\n",
    "        clf_descr = str(clf).split('(')[0]\n",
    "        return clf_descr, score, train_time, test_time\n",
    "    \n",
    "results = []\n",
    "for clf, name in (\n",
    "    (linear_model.RidgeClassifier(tol=1e-2, solver=\"sag\"), \"Ridge Classifier\"),\n",
    "    (linear_model.Perceptron(max_iter=50), \"Perceptron\"),\n",
    "    (linear_model.PassiveAggressiveClassifier(max_iter=50), \"Passive-Aggressive\"),\n",
    "    (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "    (RandomForestClassifier(), \"Random Forest\")):\n",
    "    print(\"=\"*80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "    \n",
    "for penalty in [\"l1\", \"l2\"]:\n",
    "    print(\"=\"*80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    results.append(benchmark(LinearSVC(penalty=penalty, dual=False, tol=1e-3)))\n",
    "    results.append(benchmark(linear_model.SGDClassifier(alpha=0.0001, max_iter=50, penalty=penalty)))\n",
    "    \n",
    "# train SGD with elastic net penalty\n",
    "print(\"=\"*80)\n",
    "print(\"Nearestcentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "# train sparse naive bayes classifiers\n",
    "print(\"=\"*80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=0.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=0.1)))\n",
    "results.append(benchmark(ComplementNB(alpha=.1)))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# the smaller the C, the stronger the regularization\n",
    "# the more regularization, the more the sparsity\n",
    "results.append(benchmark(Pipeline([\n",
    "    ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False, tol=1e-3))),\n",
    "    ('classification', LinearSVC(penalty=\"l2\"))\n",
    "])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RidgeClassifier',\n",
       "  0.8965262379896526,\n",
       "  0.17455697059631348,\n",
       "  0.0016829967498779297),\n",
       " ('Perceptron', 0.8876570583887657, 0.03737020492553711, 0.003830432891845703),\n",
       " ('PassiveAggressiveClassifier',\n",
       "  0.9024390243902439,\n",
       "  0.0685570240020752,\n",
       "  0.004080533981323242),\n",
       " None,\n",
       " None,\n",
       " ('LinearSVC', 0.8728750923872876, 0.193772554397583, 0.001447916030883789),\n",
       " ('SGDClassifier',\n",
       "  0.8832224685883222,\n",
       "  0.2257215976715088,\n",
       "  0.003748655319213867),\n",
       " ('LinearSVC', 0.9002217294900222, 0.4277362823486328, 0.002930164337158203),\n",
       " ('SGDClassifier',\n",
       "  0.9024390243902439,\n",
       "  0.06769657135009766,\n",
       "  0.003930330276489258),\n",
       " None,\n",
       " ('MultinomialNB',\n",
       "  0.8994826311899483,\n",
       "  0.009933948516845703,\n",
       "  0.003675699234008789),\n",
       " ('BernoulliNB',\n",
       "  0.877309682187731,\n",
       "  0.011932134628295898,\n",
       "  0.011772871017456055),\n",
       " ('ComplementNB',\n",
       "  0.9105691056910569,\n",
       "  0.010434389114379883,\n",
       "  0.0036454200744628906),\n",
       " None]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add Plots\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "pt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .2, label=\"score\", color=\"navy\")\n",
    "plt.barh(indices + .3, training_time, .2, label=\"training_time\", color=\"c\")\n",
    "plt.barh(indices + .6, test_time, .2, label=\"test time\", color=\"darkorange\")\n",
    "plt.yticks()\n",
    "plt.legend(loc=\"best\")\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
